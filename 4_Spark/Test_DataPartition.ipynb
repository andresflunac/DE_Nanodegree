{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ETL with partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, desc\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, to_date, monotonically_increasing_id\n",
    "from pyspark.sql.types import TimestampType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Extract data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All folders were deleted\n"
     ]
    }
   ],
   "source": [
    "song_folder = \"./data/song_data\"\n",
    "log_folder = \"./data/log_data\"\n",
    "songs_table_folder = \"./data/songs_table\"\n",
    "artists_table_folder = \"./data/artists_table\"\n",
    "users_table_folder = \"./data/users_table\"\n",
    "time_table_folder = \"./data/time_table\"\n",
    "songplays_table_folder = \"./data/songplays_table\"\n",
    "try:\n",
    "    shutil.rmtree(song_folder)\n",
    "    shutil.rmtree(log_folder)\n",
    "    shutil.rmtree(songs_table_folder)\n",
    "    shutil.rmtree(artists_table_folder)\n",
    "    shutil.rmtree(users_table_folder)\n",
    "    shutil.rmtree(time_table_folder)\n",
    "    shutil.rmtree(songplays_table_folder)\n",
    "except:\n",
    "    print(\"All folders were deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract Song Data\n",
    "song_zip = \"./data/song-data.zip\"\n",
    "with zipfile.ZipFile(song_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract Log Data\n",
    "log_zip = \"./data/log-data.zip\"\n",
    "with zipfile.ZipFile(log_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Process Song Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read song data file\n",
    "song_files = song_folder + \"/*/*/*\"\n",
    "df = spark.read.json(song_files)\n",
    "print(df.count())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Creating Songs Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create songs table\n",
    "song_columns = ['song_id','title','artist_id','year','duration']\n",
    "song_primary_key = ['song_id']\n",
    "song_table = df.select(song_columns).dropDuplicates(song_primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "partition_columns = [\"year\", \"artist_id\"]\n",
    "song_table.write.partitionBy(partition_columns).parquet(songs_table_folder, mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Creating Artists Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create artists table\n",
    "artist_columns = ['artist_id','artist_name','artist_location','artist_latitude','artist_longitude']\n",
    "artist_primary_key = ['artist_id']\n",
    "artist_table = df.select(artist_columns).dropDuplicates(artist_primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artist_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write artists table to parquet files\n",
    "artist_table.write.parquet(artists_table_folder, mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Process Log Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8056\n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read log data file\n",
    "log_df = spark.read.json(log_folder)\n",
    "print(log_df.count())\n",
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6820\n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter by actions for song plays\n",
    "log_df = log_df.where(\"page == 'NextSong'\")\n",
    "print(log_df.count())\n",
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Creating Users Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns for users table\n",
    "user_columns = ['userId','firstName','lastName','gender','level']\n",
    "user_primary_key = ['userId']\n",
    "users_table = log_df.select(user_columns).dropDuplicates(user_primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write users table to parquet files\n",
    "users_table.write.parquet(users_table_folder,mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Creating Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Function to convert timestamp to datetime\n",
    "@udf(TimestampType())\n",
    "def get_timestamp(ts):\n",
    "    return datetime.fromtimestamp(ts/1e3)\n",
    "# extract columns for time table\n",
    "time_columns = ['ts']\n",
    "time_primary_key = ['ts']\n",
    "time_table = log_df.select(time_columns).dropDuplicates(time_primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: long (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create additional time columns\n",
    "time_table = time_table.withColumn('month',month(get_timestamp('ts'))).withColumn('year',year(get_timestamp('ts'))).withColumn('day',dayofmonth(get_timestamp('ts'))).withColumn('hour',hour(get_timestamp('ts'))).withColumn('week',weekofyear(get_timestamp('ts'))).withColumn('weekday',date_format(get_timestamp('ts'),\"u\"))\n",
    "time_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write time table to parquet files partitioned by year and month\n",
    "partition_columns = [\"year\", \"month\"]\n",
    "time_table.write.partitionBy(partition_columns).parquet(time_table_folder, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Creating Songplays Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read song data file\n",
    "song_files = song_folder + \"/*/*/*\"\n",
    "song_df = spark.read.json(song_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Filter columns of song dataset\n",
    "songplays_columns_song = ['artist_id','artist_name','song_id','title']\n",
    "filtered_song = song_df.select(songplays_columns_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Filter columns of log dataset\n",
    "songplays_columns_log = ['ts','userId','level','song','artist','sessionId','location','userAgent']\n",
    "filtered_log = log_df.select(songplays_columns_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create additional columns\n",
    "filtered_log = filtered_log.withColumn('month',month(get_timestamp('ts'))).withColumn('year',year(get_timestamp('ts')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create songplays table \n",
    "filtered_song.createOrReplaceTempView(\"song_view\")\n",
    "filtered_log.createOrReplaceTempView(\"log_view\")\n",
    "songplays_table = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        l.ts as ts, \n",
    "        l.userId as user_id, \n",
    "        l.level as level, \n",
    "        s.song_id as song_id,\n",
    "        s.artist_id as artist_id,\n",
    "        l.sessionId as session_id,\n",
    "        l.location as location,\n",
    "        l.userAgent as user_agent,\n",
    "        l.month as month,\n",
    "        l.year as year\n",
    "    FROM log_view l\n",
    "    JOIN song_view s\n",
    "    ON l.song = s.title AND l.artist = s.artist_name\n",
    "    \"\"\"\n",
    ").withColumn(\"songplay_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- session_id: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- songplay_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "partition_columns = [\"year\", \"month\"]\n",
    "songplays_table.write.partitionBy(partition_columns).parquet(songplays_table_folder, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!rm -r ./data/spark-warehouse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
